
import os,sys
from dotenv import load_dotenv
import openai
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma

load_dotenv()
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
if OPENAI_API_KEY is None:
    raise ValueError("í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
openai.api_key = OPENAI_API_KEY

CUR_DIR = os.path.dirname(os.path.abspath(__file__))
CHROMA_PERSIST_DIR = os.path.join(CUR_DIR, 'db/test')
#CHROMA_COLLECTION_NAME = "RAG_test"

def create_prompt(query: dict):
    # ê° ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ìˆëŠ” ë³¸ë¬¸ 3ê°œë¥¼ ê°€ì ¸ì˜´
    problem = query_vectordb(query["background"], use_retriever=True, purpose="background")
    client = query_vectordb(query["target"], use_retriever=True, purpose="target")
    needs = query_vectordb(query["solution"], use_retriever=True, purpose="solution")
    trend = query_vectordb(query["assistance"], use_retriever=True, purpose="assistance")
    
    result = problem + client + needs + trend

    # ê²°ê³¼ ì¶œë ¥
    print(result[0].page_content)

    # ê° ë¬¸ì„œì˜ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì—¬ ë¬¸ìì—´ë¡œ ì—°ê²°
    docs_text = "\n\n".join([doc.page_content for doc in result])
    
    # system_message êµ¬ì„±
    system_message = f"""
    ################################################
    DBì— ì €ì¥ëœ ë¬¸ì„œì™€ ê´€ë ¨ëœ ë‹µë³€
    Documents: 
    {docs_text}
    ################################################
    """

    # user_content = f"""
    #     ë‹¹ì‹ ì€ ê´‘ê³  ë° ë§ˆì¼€íŒ… ì „ëµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

    #     ë‹¤ìŒì€ ë¬¸ì œì •ì˜, íƒ€ê²Ÿ, ì†”ë£¨ì…˜, íŠ¸ë Œë“œ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ì¶”ì¶œëœ ê´€ë ¨ ë¬¸ì„œë“¤ì…ë‹ˆë‹¤.
    #     ì´ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì‘ë‹µí•´ ì£¼ì„¸ìš”:

    #     1. ë¬¸ì„œ ë‚´ìš©ì„ ì ê·¹ì ìœ¼ë¡œ ì¸ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.
    #     2. ë¬¸ì„œì— ëª…í™•í•œ ë‚´ìš©ì´ ì—†ë‹¤ë©´, ê·¸ ì‚¬ì‹¤ì„ ëª…í™•íˆ ë°íˆê³  ìƒì‹ì  ê¸°ë°˜ìœ¼ë¡œ ë³´ì™„ ì„¤ëª…í•©ë‹ˆë‹¤.
    #     3. ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
    #     4. ë§ˆì§€ë§‰ì— 'ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì½˜ì…‰íŠ¸ ë¬¸êµ¬'ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. (ë¬¸ì„œ ê¸°ë°˜)
    #     5. ì´ 3ê°€ì§€ í˜•ì‹ìœ¼ë¡œ ê°ê° ë‹¤ë¥¸ ìŠ¤íƒ€ì¼ë¡œ ë³€í˜•í•˜ì—¬ ì œì‹œí•˜ì„¸ìš”.
    #     - âœ… **í˜•ì‹ 1**: í•µì‹¬ ìš”ì•½í˜• ìŠ¬ë¡œê±´ (10ì ë‚´ì™¸)
    #     - âœ… **í˜•ì‹ 2**: ê´‘ê³  ì¹´í”¼í˜• (1~2ë¬¸ì¥, ê°ì„± ê°•ì¡°)
    #     - âœ… **í˜•ì‹ 3**: ì „ëµì  ì„¤ëª…í˜• (2~4ë¬¸ì¥, ë…¼ë¦¬ ê¸°ë°˜)

    #     ğŸ’¡ ì‚¬ìš©ì ì§ˆë¬¸:
    #     \"\"\"{query}\"\"\"
    #     """
    
    user_content = f"""
        User question: "{str(query)}". 
        Answer the question based on the DB-RAG documents above.
        If it is in the document, actively refer to the document and create an answer. 
        If it is not in the document, inform the user that it is not in the document and create an appropriate answer.
        
        Answer in Korean.
        Finally, refer to the document to create the most appropriate Comunication concept phrase.
        Give at least three different variations of the answer in different formats.
    """
    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_content},
    ]
    return messages

def query_vectordb(query: str, purpose: str, use_retriever: bool = False):
    from pprint import pprint

    # vecotordb ë¡œë“œ
    vectordb_A = Chroma(
        persist_directory=CHROMA_PERSIST_DIR,
        embedding_function=OpenAIEmbeddings(),
        collection_name="RAG_A",
    )
    
    vectordb_B = Chroma(
        persist_directory=CHROMA_PERSIST_DIR,
        embedding_function=OpenAIEmbeddings(),
        collection_name="RAG_B",
    )

    # ë²¡í„° DBì—ì„œ ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ìˆëŠ” ë³¸ë¬¸ 3ê°œë¥¼ ê°€ì ¸ì˜´
    if use_retriever:
        print("\nuse retriever\n")
        retriever = vectordb_B.as_retriever(search_kwargs={"k": 3}, filter={"section":{purpose}})
        top_docs = retriever.get_relevant_documents(query)
        
        # idxë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•„í„°ë§
        idx_list = [doc.metadata["idx"] for doc in top_docs]
        print(f"\nidx_list: {idx_list}\n")
        all_docs = vectordb_A.similarity_search(query, k=5)  # ì¶©ë¶„íˆ ê°€ì ¸ì˜¤ê¸°
        top_docs = [doc for doc in all_docs if doc.metadata.get("idx") in idx_list]
        
        '''retriever = vectordb_A.as_retriever(
            search_kwargs={"k": 3},
            filter={"idx":{"$in":idx_list}}
        )
        top_docs = retriever.get_relevant_documents(query)'''
        
    else:
        print("\nuse similarity search\n")
        
        top_docs = vectordb_A.similarity_search(query, k=3)

    pprint(top_docs)
    return top_docs


messages = []
user_content = []

while True:
    try: 
        problem = input("ë¬¸ì œ ì •ì˜: ").strip()
        user_content.append(problem.strip())

        client = input("íƒ€ê²Ÿ ì„¤ì •: ").strip()
        user_content.append(client.strip())

        needs = input("ë‹ˆì¦ˆ íŒŒì•…: ").strip()
        user_content.append(needs.strip())

        trend = input("íŠ¸ë Œë“œ ë¶„ì„: ").strip()
        user_content.append(trend.strip())

    except (KeyboardInterrupt, EOFError):
        print("\nì…ë ¥ ì¢…ë£Œ")
        break

    if not user_content:
        print("ì…ë ¥ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        continue
    
    print("\nì…ë ¥ ë‚´ìš©" + f"\n{user_content}\n")
    
    # user_contentëŠ” ë¬¸ì œì •ì˜, íƒ€ê²Ÿì„¤ì •, ë‹ˆì¦ˆíŒŒì•…, íŠ¸ë Œë“œ ë¶„ì„ ìˆœ
    if len(user_content) != 4:
        raise ValueError("user_contentëŠ” [ë¬¸ì œì •ì˜, íƒ€ê²Ÿì„¤ì •, ë‹ˆì¦ˆíŒŒì•…, íŠ¸ë Œë“œ ë¶„ì„] ìˆœì„œë¡œ 4ê°œì˜ í•­ëª©ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")

    problem_text, client_text, needs_text, trend_text = user_content

    messages = create_prompt(
        {
            "background": problem_text,
            "target": client_text,
            "solution": needs_text,
            "assistance": trend_text
        }
    )

    # GPT ì¶œë ¥
    completion = openai.ChatCompletion.create(
        model="ft:gpt-3.5-turbo-0125:personal::BWh2RT51", # ft:gpt-3.5-turbo-0125:personal::BWh2RT51 ì–¸ì–´ìœ í¬ ft:gpt-3.5-turbo-0125:personal::BXqc5T21 ì§ˆë¬¸í˜•í˜•
        messages=messages,
        temperature=1.0,    # ê¸°ì¡´ 0.4
        max_tokens=1000
    )
    assistant_content = completion.choices[0].message["content"].strip()

    messages.append({"role": "assistant", "content": f"{assistant_content}"})

    print(f"\nGPT : {assistant_content}")
