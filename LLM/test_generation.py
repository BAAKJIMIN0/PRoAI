
import os,sys
from dotenv import load_dotenv
import openai
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma

load_dotenv()
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')
if OPENAI_API_KEY is None:
    raise ValueError("í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
openai.api_key = OPENAI_API_KEY

CUR_DIR = os.path.dirname(os.path.abspath(__file__))
CHROMA_PERSIST_DIR = os.path.join(CUR_DIR, 'db/test')
#CHROMA_COLLECTION_NAME = "RAG_test"

def create_prompt(query: dict):
    # ê° ì§ˆë¬¸ê³¼ ê°€ì¥ ê´€ë ¨ìˆëŠ” ë³¸ë¬¸ 3ê°œë¥¼ ê°€ì ¸ì˜´
    problem = query_vectordb(query["background"], use_retriever=True, purpose="background")
    client = query_vectordb(query["target"], use_retriever=True, purpose="target")
    needs = query_vectordb(query["solution"], use_retriever=True, purpose="solution")
    trend = query_vectordb(query["assistance"], use_retriever=True, purpose="assistance")
    
    result = problem + client + needs + trend

    # ê²°ê³¼ ì¶œë ¥
    print(result[0].page_content)

    # ê° ë¬¸ì„œì˜ ë‚´ìš©ì„ ì¶”ì¶œí•˜ì—¬ ë¬¸ìì—´ë¡œ ì—°ê²°
    docs_text = "\n\n".join([doc.page_content for doc in result])
    
    # system_message êµ¬ì„±
    system_message = f"""
    ################################################
    DBì— ì €ì¥ëœ ë¬¸ì„œì™€ ê´€ë ¨ëœ ë‹µë³€
    Documents: 
    {docs_text}
    ################################################
    """

    # user_content = f"""
    #     ë‹¹ì‹ ì€ ê´‘ê³  ë° ë§ˆì¼€íŒ… ì „ëµ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

    #     ë‹¤ìŒì€ ë¬¸ì œì •ì˜, íƒ€ê²Ÿ, ì†”ë£¨ì…˜, íŠ¸ë Œë“œ ë¶„ì„ì„ ë°”íƒ•ìœ¼ë¡œ ì¶”ì¶œëœ ê´€ë ¨ ë¬¸ì„œë“¤ì…ë‹ˆë‹¤.
    #     ì´ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì•„ë˜ ì§ˆë¬¸ì— ëŒ€í•´ ë‹¤ìŒê³¼ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì‘ë‹µí•´ ì£¼ì„¸ìš”:

    #     1. ë¬¸ì„œ ë‚´ìš©ì„ ì ê·¹ì ìœ¼ë¡œ ì¸ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•©ë‹ˆë‹¤.
    #     2. ë¬¸ì„œì— ëª…í™•í•œ ë‚´ìš©ì´ ì—†ë‹¤ë©´, ê·¸ ì‚¬ì‹¤ì„ ëª…í™•íˆ ë°íˆê³  ìƒì‹ì  ê¸°ë°˜ìœ¼ë¡œ ë³´ì™„ ì„¤ëª…í•©ë‹ˆë‹¤.
    #     3. ì‘ë‹µì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.
    #     4. ë§ˆì§€ë§‰ì— 'ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì½˜ì…‰íŠ¸ ë¬¸êµ¬'ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. (ë¬¸ì„œ ê¸°ë°˜)
    #     5. ì´ 3ê°€ì§€ í˜•ì‹ìœ¼ë¡œ ê°ê° ë‹¤ë¥¸ ìŠ¤íƒ€ì¼ë¡œ ë³€í˜•í•˜ì—¬ ì œì‹œí•˜ì„¸ìš”.
    #     - âœ… **í˜•ì‹ 1**: í•µì‹¬ ìš”ì•½í˜• ìŠ¬ë¡œê±´ (10ì ë‚´ì™¸)
    #     - âœ… **í˜•ì‹ 2**: ê´‘ê³  ì¹´í”¼í˜• (1~2ë¬¸ì¥, ê°ì„± ê°•ì¡°)
    #     - âœ… **í˜•ì‹ 3**: ì „ëµì  ì„¤ëª…í˜• (2~4ë¬¸ì¥, ë…¼ë¦¬ ê¸°ë°˜)

    #     ğŸ’¡ ì‚¬ìš©ì ì§ˆë¬¸:
    #     \"\"\"{query}\"\"\"
    #     """
    
    # user_content = f"""
    #     User question: "{str(query)}". 
    #     Answer the question based on the DB-RAG documents above.
    #     If it is in the document, actively refer to the document and create an answer. 
    #     If it is not in the document, inform the user that it is not in the document and create an appropriate answer.
    #     Finally, refer to the document to create the most appropriate Comunication concept phrase.
    #     Answer in Korean.
    #     Give at least three different variations concepts of the answer in different formats.
    # """

    # user_content = f"""
    # User question: "{str(query)}". 
    # Please answer based on the DB-RAG documents provided above.

    # If the document clearly includes relevant content, use it in your answer.  
    # If not, infer a possible answer by combining related insights or trends from the documents.  

    # Respond in Korean. At the end, generate 3 creative variations of a communication concept based on the content.
    # """
    print("ì§ˆë¬¸ë¬¸ìì—´:",str(query))
    user_content = f"""
        ë‹¹ì‹ ì€ ê´‘ê³  ë° ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì „ëµì— ì •í†µí•œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

        ì•„ë˜ëŠ” ë¬¸ì œ ì •ì˜, íƒ€ê²Ÿ, ì†”ë£¨ì…˜, íŠ¸ë Œë“œ ë¶„ì„ì„ ê¸°ë°˜ìœ¼ë¡œ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì…ë‹ˆë‹¤.
        ì´ ë¬¸ì„œë¥¼ ì ê·¹ì ìœ¼ë¡œ ì¸ìš©í•˜ê±°ë‚˜, ëª…ì‹œì ì¸ ë‚´ìš©ì´ ì—†ì„ ê²½ìš° ë¬¸ë§¥ ê¸°ë°˜ì˜ ë…¼ë¦¬ì  ì¶”ë¡ ì„ í†µí•´ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.
        ë‹µë³€ì€ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ë˜, ì–¸ì–´ìœ í¬ ê²½ìš° ì˜ì–´ë„ í™œìš©í•´ ì£¼ì„¸ìš”.

        --- ì‚¬ìš©ì ì§ˆë¬¸ ---
        \"\"\"{str(query)}\"\"\"

        --- ìš”êµ¬ì‚¬í•­ ---
        backgroundì— í•´ë‹¹ë˜ëŠ” ë¬¸ì œì ì„ íŒŒì•…í•˜ê³ , target ì—°ë ¹ëŒ€ì— ë§ëŠ” ì»¨ì…‰ìœ¼ë¡œ, solutionì— í•´ë‹¹ë˜ëŠ” ë‚´ìš©ì— ë§ê²Œ,
        assistanceì˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬, ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ **3ê°€ì§€ ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ ì½˜ì…‰íŠ¸ ìŠ¤íƒ€ì¼**ì„ ì œì•ˆí•˜ì„¸ìš”:

        1. ë‹¤ì–‘í•œ ìŠ¤íƒ€ì¼(ë¬¸ì œí•´ê²°í˜•, íŠ¸ë Œë“œë°˜ì˜í˜•, ì°¸ì—¬ìœ ë„í˜•, ê°ì„±ì¶”êµ¬í˜•, ê´€ì‹¬ì£¼ëª©í˜•) ì¤‘ **ë¬¸ì œì— ê°€ì¥ ì í•©í•œ 3ê°€ì§€**ë¥¼ ì„ ì •í•˜ì„¸ìš”.
        2. ê° ìŠ¤íƒ€ì¼ì€ ì„œë¡œ ëª…í™•íˆ ë‹¤ë¥¸ ì ‘ê·¼ì„ ì·¨í•´ì•¼ í•˜ë©°, **GPTê°€ ë¬¸ë§¥ìƒ ê°€ì¥ íš¨ê³¼ì ì¸ ì¡°í•©**ì„ íŒë‹¨í•˜ì—¬ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.
        3. ê° ì½˜ì…‰íŠ¸ëŠ” ë‹¤ìŒì˜ 3ê°€ì§€ ìš”ì†Œë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:
            - âœ… ì½˜ì…‰íŠ¸ ìœ í˜•ëª… (ììœ ë¡­ê²Œ ì •í•˜ë˜ ëª…í™•í•  ê²ƒ)
            - âœ… í•´ë‹¹ ìŠ¤íƒ€ì¼ë¡œ ì‘ì„±í•œ ì½˜ì…‰íŠ¸ ë¬¸êµ¬ (ë°˜ë“œì‹œ 10~40ì ì‚¬ì´ì˜ ê°„ê²°í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±)
            - âœ… ê°„ë‹¨í•œ ì´ìœ  ë˜ëŠ” ëª©ì  ì„¤ëª… (ì´ ìŠ¤íƒ€ì¼ì´ ë¬¸ì œ ì •ì˜ì™€ íƒ€ê²Ÿì— ì–´ë–»ê²Œ ë¶€í•©í•˜ë©°, ì–´ë–¤ ë©”ì‹œì§€ë¥¼ ì „ë‹¬í•˜ê³ ì í•˜ëŠ”ì§€ ê°„ê²°íˆ ì„¤ëª…í•´ ì£¼ì„¸ìš”.)

        ì˜ˆì‹œ í˜•ì‹:
        1ï¸âƒ£ [ìœ í˜•ëª…]  
        ë¬¸êµ¬: "..."  
        ì„¤ëª…: ...

        ---
        ì‘ë‹µ í˜•ì‹ì€ ìœ„ ì˜ˆì‹œì™€ ìœ ì‚¬í•˜ê²Œ, ì½˜ì…‰íŠ¸ 3ê°€ì§€ë¥¼ ìš°ì„ ìˆœìœ„ ìˆœìœ¼ë¡œ ì œì‹œí•˜ì„¸ìš”.
        """



    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_content},
    ]
    return messages

def query_vectordb(query: str, purpose: str, use_retriever: bool = False):
    from pprint import pprint

    # ë²¡í„°DB ë‹¨ì¼í™”
    vectordb = Chroma(
        persist_directory=CHROMA_PERSIST_DIR,
        embedding_function=OpenAIEmbeddings(),
        collection_name="RAG_B",  # ê¸°ì¡´ Aë¡œ í†µì¼
    )
    docs = vectordb.similarity_search("ëŒ€í•™ìƒ", k=1)
    print(docs[0].metadata)
    if use_retriever:
        print("\n[retriever ì‚¬ìš©] section í•„í„°:", purpose)

        # section í•„í„° ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
        retriever = vectordb.as_retriever(
            search_kwargs={"k": 3},
            filter={"section": purpose}
        )
        top_docs = retriever.get_relevant_documents(query)

    else:
        print("\n[similarity_search ì‚¬ìš©]")

        # ë‹¨ìˆœ ë²¡í„° ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰
        top_docs = vectordb.similarity_search(query, k=3)

    pprint(top_docs)
    return top_docs


messages = []
user_content = []

while True:
    try: 
        problem = input("ë¬¸ì œ ì •ì˜: ").strip()
        user_content.append(problem.strip())

        client = input("íƒ€ê²Ÿ ì„¤ì •: ").strip()
        user_content.append(client.strip())

        needs = input("ë‹ˆì¦ˆ íŒŒì•…: ").strip()
        user_content.append(needs.strip())

        trend = input("íŠ¸ë Œë“œ ë¶„ì„: ").strip()
        user_content.append(trend.strip())

    except (KeyboardInterrupt, EOFError):
        print("\nì…ë ¥ ì¢…ë£Œ")
        break

    if not user_content:
        print("ì…ë ¥ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
        continue
    
    print("\nì…ë ¥ ë‚´ìš©" + f"\n{user_content}\n")
    
    # user_contentëŠ” ë¬¸ì œì •ì˜, íƒ€ê²Ÿì„¤ì •, ë‹ˆì¦ˆíŒŒì•…, íŠ¸ë Œë“œ ë¶„ì„ ìˆœ
    if len(user_content) != 4:
        raise ValueError("user_contentëŠ” [ë¬¸ì œì •ì˜, íƒ€ê²Ÿì„¤ì •, ë‹ˆì¦ˆíŒŒì•…, íŠ¸ë Œë“œ ë¶„ì„] ìˆœì„œë¡œ 4ê°œì˜ í•­ëª©ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.")

    problem_text, client_text, needs_text, trend_text = user_content

    messages = create_prompt(
        {
            "background": problem_text,
            "target": client_text,
            "solution": needs_text,
            "assistance": trend_text
        }
    )

    # GPT ì¶œë ¥
    completion = openai.ChatCompletion.create(
        model="ft:gpt-3.5-turbo-0125:personal::BWh2RT51", # ft:gpt-3.5-turbo-0125:personal::BWh2RT51 ì–¸ì–´ìœ í¬ ft:gpt-3.5-turbo-0125:personal::BXqc5T21 ì§ˆë¬¸í˜•í˜•
        messages=messages,
        temperature=1.0,    # ê¸°ì¡´ 0.4
        max_tokens=1000
    )
    assistant_content = completion.choices[0].message["content"].strip()

    messages.append({"role": "assistant", "content": f"{assistant_content}"})

    print(f"\nGPT : {assistant_content}")
